#!/usr/bin/env python3
"""
################################################################################
#                         Loki Benchmark Test Suite
################################################################################
#
# ü§ñ This benchmark test was generated by GPT (Claude Sonnet 4) to provide
#    comprehensive Loki performance testing for both write and read operations.
#
# üìã DESCRIPTION:
#    A professional-grade benchmark testing tool that measures Loki
#    performance across multiple dimensions:
#    - Log ingestion throughput (write performance)
#    - Query performance (read operations)
#    - Concurrent client handling
#    - Different log formats and sizes
#    - Label cardinality impact
#    - Time range query performance
#    - Stream distribution efficiency
#
# üîß PREREQUISITES:
#    - Python 3.6 or higher
#    - Loki server running and accessible
#    - Python requests library (pip install requests)
#    - Network access to Loki API endpoints
#    - Sufficient disk space for log storage on Loki server
#
# üì¶ INSTALLATION:
#    pip install requests
#    # OR using system package manager:
#    # Ubuntu/Debian: sudo apt-get install python3-requests
#    # CentOS/RHEL: sudo yum install python3-requests
#    # macOS: pip3 install requests
#
# üöÄ BASIC USAGE:
#    # Start Loki port forwarding first (if using Kubernetes):
#    kubectl port-forward svc/lokicluster-gateway 8080:80
#
#    # Run basic benchmark
#    python3 loki_benchmark.py --url http://localhost:8080
#
# üéØ ADVANCED USAGE EXAMPLES:
#
#    # Quick test with small dataset
#    python3 loki_benchmark.py --url http://localhost:8080 \
#                             --clients 5 --logs-per-client 100 --duration 30
#
#    # High-throughput test with large dataset
#    python3 loki_benchmark.py --url http://localhost:8080 \
#                             --clients 50 --logs-per-client 10000 --batch-size 500
#
#    # Custom log format and size testing
#    python3 loki_benchmark.py --url http://localhost:8080 \
#                             --log-size 1024 --label-cardinality 100
#
#    # Read-heavy benchmark focusing on queries
#    python3 loki_benchmark.py --url http://localhost:8080 --read-only
#
# üìä WHAT IT MEASURES:
#    ‚úÖ Log ingestion rate (logs/second)
#    ‚úÖ Data throughput (bytes/second)
#    ‚úÖ Query response time and throughput
#    ‚úÖ Concurrent client handling
#    ‚úÖ Error rates and retry behavior
#    ‚úÖ Memory usage patterns
#    ‚úÖ Label cardinality impact on performance
#    ‚úÖ Time range query efficiency
#    ‚úÖ Stream distribution and indexing performance
#    ‚úÖ Network latency and connection pooling
#
# üìà OUTPUT:
#    - Real-time progress with emoji indicators
#    - Comprehensive console report with key metrics
#    - JSON file export with detailed results for analysis
#    - Performance graphs and trend analysis
#    - Error analysis and retry statistics
#    - Resource utilization metrics
#
# üóÇÔ∏è TEST DATA:
#    The benchmark generates realistic log data with:
#    - Structured JSON logs
#    - Variable message sizes
#    - Realistic timestamps
#    - Configurable label cardinality
#    - Different log levels and formats
#
# ‚ö†Ô∏è  SAFETY NOTES:
#    - Creates temporary log streams for testing
#    - Configurable retention policies
#    - Rate limiting to prevent server overload
#    - Graceful error handling and cleanup
#    - Safe to run against production Loki (with appropriate limits)
#
# üèÜ LOKI PERFORMANCE TIPS:
#    - Optimize chunk_target_size for your log volume
#    - Use appropriate retention policies
#    - Configure proper label cardinality (avoid high cardinality)
#    - Use structured logging with consistent labels
#    - Monitor ingester memory usage
#    - Consider using parallel queries for large time ranges
#    - Use appropriate compression settings
#    - Configure proper replication factor for your use case
#
# üìÑ GENERATED REPORTS:
#    - Console output: Real-time results with summary statistics
#    - JSON file: loki_benchmark_YYYYMMDD_HHMMSS.json with detailed metrics
#    - Metrics include: ingestion rate, query latency, error rates, resource usage
#
# üêõ TROUBLESHOOTING:
#    - "Connection refused" ‚Üí Check Loki URL and port forwarding
#    - "429 Too Many Requests" ‚Üí Reduce concurrent clients or batch size
#    - "413 Request Entity Too Large" ‚Üí Reduce batch size or log size
#    - High memory usage ‚Üí Reduce concurrent operations
#    - Query timeouts ‚Üí Check time range and label selectors
#    - "No data found" ‚Üí Ensure logs are being ingested properly
#
# üìö LOKI-SPECIFIC NOTES:
#    - Tests both push and query APIs
#    - Supports LogQL query language testing
#    - Compatible with Grafana Loki 2.x and 3.x
#    - Tests label indexing efficiency
#    - Measures chunk compression effectiveness
#    - Validates stream limits and cardinality
#
# üåê KUBERNETES SETUP:
#    # Port forward Loki service
#    kubectl port-forward svc/loki 3100:3100
#
#    # Or for Loki with gateway
#    kubectl port-forward svc/loki-gateway 8080:80
#
#    # Then run benchmark
#    python3 loki_benchmark.py --url http://localhost:3100
#
# üìû SUPPORT:
#    This tool was generated by AI and is provided as-is. For Loki
#    performance tuning, consult the official Grafana Loki documentation
#    at grafana.com/docs/loki/.
#
################################################################################

A comprehensive benchmark testing tool for Loki that measures:
- Log ingestion performance
- Query performance and latency
- Concurrent client handling
- Label cardinality impact
- Time range query efficiency

Requirements:
- Python 3.6+
- requests library
- Access to Loki API

Usage:
    python3 loki_benchmark.py --url http://localhost:3100
"""

import requests
import time
import random
import string
import json
import argparse
import sys
import statistics
import threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin


class LokiBenchmark:
    def __init__(self, loki_url, timeout=30):
        self.loki_url = loki_url.rstrip('/')
        self.push_url = urljoin(self.loki_url, '/loki/api/v1/push')
        self.query_url = urljoin(self.loki_url, '/loki/api/v1/query_range')
        self.labels_url = urljoin(self.loki_url, '/loki/api/v1/labels')
        self.timeout = timeout
        self.session = requests.Session()
        self.results = {
            'start_time': datetime.now().isoformat(),
            'loki_url': loki_url,
            'write_tests': {},
            'read_tests': {},
            'errors': []
        }

    def _random_string(self, length=10):
        """Generate a random string for log content"""
        return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))

    def _generate_log_entry(self, timestamp=None, size=100, level=None):
        """Generate a realistic log entry"""
        if timestamp is None:
            timestamp = time.time_ns()

        if level is None:
            level = random.choice(['INFO', 'WARN', 'ERROR', 'DEBUG'])

        # Generate realistic log content
        log_data = {
            'timestamp': datetime.fromtimestamp(timestamp / 1e9).isoformat(),
            'level': level,
            'service': random.choice(['api', 'worker', 'scheduler', 'database']),
            'message': self._random_string(size // 2),
            'user_id': random.randint(1000, 9999),
            'request_id': self._random_string(8),
            'duration_ms': random.randint(1, 1000),
        }

        # Convert to log line format
        log_line = json.dumps(log_data)

        # Pad to desired size if needed
        if len(log_line) < size:
            padding = self._random_string(size - len(log_line))
            log_data['padding'] = padding
            log_line = json.dumps(log_data)

        return timestamp, log_line[:size]  # Ensure exact size

    def _generate_labels(self, cardinality=10, client_id=0):
        """Generate labels for log streams"""
        base_labels = {
            'job': 'benchmark_test',
            'client': f'client_{client_id}',
        }

        # Add variable labels based on cardinality
        if cardinality > 10:
            base_labels['instance'] = f'instance_{random.randint(0, min(cardinality//10, 100))}'
        if cardinality > 50:
            base_labels['pod'] = f'pod_{random.randint(0, min(cardinality//5, 200))}'
        if cardinality > 100:
            base_labels['node'] = f'node_{random.randint(0, min(cardinality//20, 50))}'

        return base_labels

    def check_connection(self):
        """Test basic connectivity to Loki"""
        print("üîå Testing Loki connection...")

        try:
            start_time = time.time()
            response = self.session.get(
                urljoin(self.loki_url, '/ready'),
                timeout=self.timeout
            )
            connection_time = time.time() - start_time

            if response.status_code == 200:
                print(f"‚úÖ Connected successfully in {connection_time:.3f}s")

                # Try to get Loki version/build info
                try:
                    build_info = self.session.get(
                        urljoin(self.loki_url, '/loki/api/v1/status/buildinfo'),
                        timeout=5
                    )
                    if build_info.status_code == 200:
                        info = build_info.json()
                        version = info.get('version', 'Unknown')
                        print(f"   Loki Version: {version}")
                except:
                    pass

                self.results['connection'] = {
                    'success': True,
                    'time': connection_time,
                    'status_code': response.status_code
                }
                return True
            else:
                print(f"‚ùå Connection failed with status {response.status_code}")
                self.results['connection'] = {
                    'success': False,
                    'status_code': response.status_code
                }
                return False

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Connection failed: {e}")
            self.results['connection'] = {
                'success': False,
                'error': str(e)
            }
            return False

    def benchmark_write_performance(self, num_clients=10, logs_per_client=1000,
                                   batch_size=100, log_size=200, label_cardinality=10):
        """Benchmark log ingestion performance"""
        print(f"üìä Benchmarking Write Performance")
        print(f"   Clients: {num_clients}")
        print(f"   Logs per client: {logs_per_client:,}")
        print(f"   Batch size: {batch_size}")
        print(f"   Log size: {log_size} bytes")
        print(f"   Label cardinality: {label_cardinality}")
        print("="*50)

        def write_worker(client_id):
            """Worker function for a single client"""
            worker_stats = {
                'logs_sent': 0,
                'bytes_sent': 0,
                'requests_made': 0,
                'errors': 0,
                'duration': 0,
                'error_details': []
            }

            start_time = time.time()
            batch = []
            stream_labels = self._generate_labels(label_cardinality, client_id)

            try:
                for i in range(logs_per_client):
                    timestamp, log_line = self._generate_log_entry(size=log_size)
                    batch.append([str(timestamp), log_line])

                    if len(batch) >= batch_size:
                        # Send batch
                        success, bytes_sent = self._send_batch(stream_labels, batch, worker_stats)
                        if success:
                            worker_stats['logs_sent'] += len(batch)
                            worker_stats['bytes_sent'] += bytes_sent
                        batch = []
                        worker_stats['requests_made'] += 1

                # Send remaining logs
                if batch:
                    success, bytes_sent = self._send_batch(stream_labels, batch, worker_stats)
                    if success:
                        worker_stats['logs_sent'] += len(batch)
                        worker_stats['bytes_sent'] += bytes_sent
                    worker_stats['requests_made'] += 1

            except Exception as e:
                worker_stats['errors'] += 1
                worker_stats['error_details'].append(str(e))

            worker_stats['duration'] = time.time() - start_time
            return client_id, worker_stats

        # Execute concurrent workers
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=num_clients) as executor:
            futures = [executor.submit(write_worker, i) for i in range(num_clients)]

            # Collect results
            all_stats = {}
            for future in as_completed(futures):
                client_id, stats = future.result()
                all_stats[client_id] = stats

                # Show progress
                print(f"   Client {client_id}: {stats['logs_sent']:,} logs, "
                      f"{stats['bytes_sent']:,} bytes, {stats['errors']} errors")

        total_duration = time.time() - start_time

        # Aggregate statistics
        total_logs = sum(s['logs_sent'] for s in all_stats.values())
        total_bytes = sum(s['bytes_sent'] for s in all_stats.values())
        total_requests = sum(s['requests_made'] for s in all_stats.values())
        total_errors = sum(s['errors'] for s in all_stats.values())

        logs_per_second = total_logs / total_duration if total_duration > 0 else 0
        bytes_per_second = total_bytes / total_duration if total_duration > 0 else 0

        write_results = {
            'clients': num_clients,
            'logs_per_client': logs_per_client,
            'batch_size': batch_size,
            'log_size': log_size,
            'label_cardinality': label_cardinality,
            'total_logs': total_logs,
            'total_bytes': total_bytes,
            'total_requests': total_requests,
            'total_errors': total_errors,
            'duration': total_duration,
            'logs_per_second': logs_per_second,
            'bytes_per_second': bytes_per_second,
            'mbytes_per_second': bytes_per_second / (1024 * 1024),
            'error_rate': (total_errors / total_requests * 100) if total_requests > 0 else 0,
            'client_stats': all_stats
        }

        print(f"\n‚úÖ Write Performance Results:")
        print(f"   Total logs sent: {total_logs:,}")
        print(f"   Total data sent: {total_bytes / (1024*1024):.2f} MB")
        print(f"   Duration: {total_duration:.2f}s")
        print(f"   Ingestion rate: {logs_per_second:.0f} logs/second")
        print(f"   Throughput: {bytes_per_second / (1024*1024):.2f} MB/second")
        print(f"   Error rate: {write_results['error_rate']:.2f}%")

        self.results['write_tests']['performance'] = write_results
        return write_results

    def _send_batch(self, labels, logs_batch, stats):
        """Send a batch of logs to Loki"""
        payload = {
            'streams': [{
                'stream': labels,
                'values': logs_batch
            }]
        }

        try:
            response = self.session.post(
                self.push_url,
                json=payload,
                timeout=self.timeout
            )

            if response.status_code == 204:
                # Calculate approximate bytes sent
                bytes_sent = len(json.dumps(payload).encode('utf-8'))
                return True, bytes_sent
            else:
                stats['errors'] += 1
                stats['error_details'].append(f"HTTP {response.status_code}: {response.text}")
                return False, 0

        except requests.exceptions.RequestException as e:
            stats['errors'] += 1
            stats['error_details'].append(str(e))
            return False, 0

    def benchmark_read_performance(self, queries_count=100, time_range_hours=1):
        """Benchmark query performance"""
        print(f"\nüìä Benchmarking Read Performance")
        print(f"   Queries to execute: {queries_count}")
        print(f"   Time range: {time_range_hours} hour(s)")
        print("="*50)

        # Wait a bit for logs to be indexed
        print("‚è≥ Waiting for logs to be indexed...")
        time.sleep(10)

        end_time = datetime.now()
        start_time = end_time - timedelta(hours=time_range_hours)

        # Convert to nanoseconds (Loki's required format)
        start_ns = int(start_time.timestamp() * 1e9)
        end_ns = int(end_time.timestamp() * 1e9)

        # Define various query patterns
        query_patterns = [
            ('{job="benchmark_test"}', 'All benchmark logs'),
            ('{job="benchmark_test"} |= "ERROR"', 'Error logs'),
            ('{job="benchmark_test"} | json | level="INFO"', 'Info level logs'),
            ('count_over_time({job="benchmark_test"}[5m])', 'Count over time'),
            ('rate({job="benchmark_test"}[1m])', 'Rate calculation'),
        ]

        query_results = {}

        for query, description in query_patterns:
            print(f"\nüîπ Testing: {description}")
            print(f"   Query: {query}")

            times = []
            results_count = []
            errors = 0

            for i in range(min(queries_count // len(query_patterns), 20)):
                start_query_time = time.time()

                try:
                    params = {
                        'query': query,
                        'start': str(start_ns),
                        'end': str(end_ns),
                        'limit': 1000
                    }

                    response = self.session.get(
                        self.query_url,
                        params=params,
                        timeout=self.timeout
                    )

                    query_time = time.time() - start_query_time

                    if response.status_code == 200:
                        data = response.json()
                        result_count = 0

                        if 'data' in data and 'result' in data['data']:
                            for result in data['data']['result']:
                                if 'values' in result:
                                    result_count += len(result['values'])

                        times.append(query_time)
                        results_count.append(result_count)

                        if i == 0:  # Show first result
                            print(f"   First query: {query_time:.3f}s, {result_count} results")
                    else:
                        errors += 1
                        print(f"   ‚ùå Query failed: {response.status_code} - {response.text}")

                except Exception as e:
                    errors += 1
                    print(f"   ‚ùå Query error: {e}")

            if times:
                avg_time = statistics.mean(times)
                min_time = min(times)
                max_time = max(times)
                avg_results = statistics.mean(results_count) if results_count else 0

                query_results[description] = {
                    'query': query,
                    'avg_time': avg_time,
                    'min_time': min_time,
                    'max_time': max_time,
                    'avg_results': avg_results,
                    'total_queries': len(times),
                    'errors': errors
                }

                print(f"   üìä Avg: {avg_time:.3f}s, Min: {min_time:.3f}s, Max: {max_time:.3f}s")
                print(f"   üìà Avg results: {avg_results:.0f}")

        self.results['read_tests']['performance'] = query_results
        print(f"\n‚úÖ Read Performance testing completed")

        return query_results

    def benchmark_label_cardinality_impact(self, cardinality_levels=[10, 50, 100, 500]):
        """Test the impact of label cardinality on performance"""
        print(f"\nüìä Benchmarking Label Cardinality Impact")
        print(f"   Cardinality levels: {cardinality_levels}")
        print("="*50)

        cardinality_results = {}

        for cardinality in cardinality_levels:
            print(f"\nüîπ Testing cardinality: {cardinality}")

            # Small write test for each cardinality level
            result = self.benchmark_write_performance(
                num_clients=5,
                logs_per_client=200,
                batch_size=50,
                log_size=100,
                label_cardinality=cardinality
            )

            cardinality_results[cardinality] = {
                'logs_per_second': result['logs_per_second'],
                'mbytes_per_second': result['mbytes_per_second'],
                'error_rate': result['error_rate']
            }

            print(f"   üìä {cardinality} labels: {result['logs_per_second']:.0f} logs/sec")

        self.results['cardinality_tests'] = cardinality_results
        return cardinality_results

    def generate_report(self):
        """Generate comprehensive benchmark report"""
        print("\n" + "="*80)
        print("üìã LOKI BENCHMARK REPORT")
        print("="*80)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Loki URL: {self.loki_url}")
        print("-"*80)

        # Connection info
        if 'connection' in self.results:
            conn = self.results['connection']
            print(f"Connection Test: {'‚úÖ PASS' if conn['success'] else '‚ùå FAIL'}")
            if conn['success']:
                print(f"  Connection Time: {conn['time']:.3f}s")

        # Write performance
        if 'write_tests' in self.results and 'performance' in self.results['write_tests']:
            write = self.results['write_tests']['performance']
            print(f"\nüìä WRITE PERFORMANCE:")
            print(f"  Total Logs: {write['total_logs']:,}")
            print(f"  Data Volume: {write['total_bytes'] / (1024*1024):.2f} MB")
            print(f"  Duration: {write['duration']:.2f}s")
            print(f"  Ingestion Rate: {write['logs_per_second']:.0f} logs/second")
            print(f"  Throughput: {write['mbytes_per_second']:.2f} MB/second")
            print(f"  Error Rate: {write['error_rate']:.2f}%")

        # Read performance
        if 'read_tests' in self.results and 'performance' in self.results['read_tests']:
            print(f"\nüìä READ PERFORMANCE:")
            for query_name, stats in self.results['read_tests']['performance'].items():
                print(f"  {query_name}:")
                print(f"    Avg Time: {stats['avg_time']:.3f}s")
                print(f"    Avg Results: {stats['avg_results']:.0f}")

        # Cardinality impact
        if 'cardinality_tests' in self.results:
            print(f"\nüìä LABEL CARDINALITY IMPACT:")
            print("  Labels | Logs/sec | MB/sec | Error%")
            print("  " + "-"*35)
            for cardinality, stats in self.results['cardinality_tests'].items():
                print(f"   {cardinality:4d}  | {stats['logs_per_second']:8.0f} | "
                      f"{stats['mbytes_per_second']:6.2f} | {stats['error_rate']:5.1f}%")

        print("\n" + "="*80)

        # Save detailed results to JSON
        report_file = f"loki_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(report_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            print(f"üìÑ Detailed results saved to: {report_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not save detailed results: {e}")

    def run_full_benchmark(self, num_clients=10, logs_per_client=1000,
                          batch_size=100, log_size=200, read_only=False):
        """Run the complete benchmark suite"""
        start_time = time.time()

        print("üöÄ Starting Loki Benchmark Suite")
        print("="*60)

        # # Check connection
        # if not self.check_connection():
        #     print("‚ùå Cannot proceed without Loki connection")
        #     return False

        try:
            if not read_only:
                # Write performance test
                self.benchmark_write_performance(
                    num_clients=num_clients,
                    logs_per_client=logs_per_client,
                    batch_size=batch_size,
                    log_size=log_size
                )

                # Label cardinality impact test
                self.benchmark_label_cardinality_impact()

            # Read performance test
            self.benchmark_read_performance()

        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Benchmark interrupted by user")
        except Exception as e:
            print(f"\n‚ùå Unexpected error during benchmark: {e}")
            self.results['errors'].append(str(e))

        total_time = time.time() - start_time
        print(f"\n‚è±Ô∏è  Total benchmark time: {total_time:.1f} seconds")

        # Generate report
        self.generate_report()
        return True


def main():
    parser = argparse.ArgumentParser(
        description='Loki Benchmark Test Suite',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic benchmark (requires port forwarding)
  kubectl port-forward svc/loki 3100:3100
  python3 loki_benchmark.py --url http://localhost:3100

  # High-throughput test
  python3 loki_benchmark.py --url http://localhost:3100 \\
                           --clients 50 --logs-per-client 5000 --batch-size 500

  # Read-only benchmark
  python3 loki_benchmark.py --url http://localhost:3100 --read-only
        """
    )

    parser.add_argument('--url', required=True, help='Loki URL (e.g., http://localhost:3100)')
    parser.add_argument('--clients', type=int, default=10, help='Number of concurrent clients (default: 10)')
    parser.add_argument('--logs-per-client', type=int, default=1000, help='Logs per client (default: 1000)')
    parser.add_argument('--batch-size', type=int, default=100, help='Batch size for log ingestion (default: 100)')
    parser.add_argument('--log-size', type=int, default=200, help='Log entry size in bytes (default: 200)')
    parser.add_argument('--timeout', type=int, default=30, help='Request timeout in seconds (default: 30)')
    parser.add_argument('--read-only', action='store_true', help='Run only read performance tests')

    args = parser.parse_args()

    # Validate arguments
    if args.clients <= 0 or args.logs_per_client <= 0 or args.batch_size <= 0:
        print("‚ùå Error: Client count, logs per client, and batch size must be positive")
        sys.exit(1)

    if args.log_size < 50:
        print("‚ùå Error: Log size must be at least 50 bytes")
        sys.exit(1)

    # Check requests library
    try:
        import requests
    except ImportError:
        print("‚ùå Error: requests library is required")
        print("Install with: pip install requests")
        sys.exit(1)

    # Run benchmark
    benchmark = LokiBenchmark(args.url, timeout=args.timeout)

    success = benchmark.run_full_benchmark(
        num_clients=args.clients,
        logs_per_client=args.logs_per_client,
        batch_size=args.batch_size,
        log_size=args.log_size,
        read_only=args.read_only
    )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()