#!/usr/bin/env python3
"""
################################################################################
#                         Redis Benchmark Test Suite
################################################################################
#
# ü§ñ This comprehensive Redis benchmark tool was generated and refined by GPT
#    (Claude Sonnet 4) to provide professional-grade Redis performance testing.
#
# üìã DESCRIPTION:
#    A professional Redis benchmark tool that measures performance across
#    multiple operations and scenarios:
#    - Basic operations: SET, GET, DEL, EXISTS
#    - List operations: LPUSH, LPOP, RPUSH, RPOP
#    - Hash operations: HSET, HGET, HDEL
#    - Set operations: SADD, SMEMBERS, SREM
#    - Pipeline operations for bulk testing
#    - Concurrent client simulation
#    - Latency percentile analysis
#    - Memory usage monitoring
#    - Connection pooling efficiency
#
# üîß PREREQUISITES:
#    - Python 3.6 or higher
#    - Redis server running and accessible
#    - Python redis library (pip install redis)
#    - Network access to Redis instance
#    - Sufficient memory for test data
#
# üì¶ INSTALLATION:
#    pip install redis
#    # OR using system package manager:
#    # Ubuntu/Debian: sudo apt-get install python3-redis
#    # CentOS/RHEL: sudo yum install python3-redis
#    # macOS: pip3 install redis
#
# üöÄ BASIC USAGE:
#    # Local Redis instance
#    python3 redis_benchmark.py --host localhost --port 6379
#
#    # Kubernetes Redis service (with port forwarding)
#    kubectl port-forward svc/redis 6379:6379
#    python3 redis_benchmark.py --host localhost --port 6379
#
# üéØ ADVANCED USAGE EXAMPLES:
#
#    # High-throughput test with many clients
#    python3 redis_benchmark.py --host localhost --port 6379 \
#                              --clients 200 --requests 1000000 --value-size 1024
#
#    # Authentication with username/password
#    python3 redis_benchmark.py --host redis.example.com --port 6379 \
#                              --username myuser --password mypass
#
#    # Pipeline mode for maximum throughput
#    python3 redis_benchmark.py --host localhost --port 6379 \
#                              --pipeline --pipeline-size 100
#
#    # Test specific operations only
#    python3 redis_benchmark.py --host localhost --port 6379 \
#                              --operations SET,GET,LPUSH,LPOP
#
#    # Export results to JSON
#    python3 redis_benchmark.py --host localhost --port 6379 \
#                              --output redis_results.json
#
# üìä WHAT IT MEASURES:
#    ‚úÖ Operations per second (throughput)
#    ‚úÖ Latency statistics (mean, median, p95, p99)
#    ‚úÖ Memory usage patterns
#    ‚úÖ Connection establishment time
#    ‚úÖ Error rates and types
#    ‚úÖ Pipeline efficiency
#    ‚úÖ Concurrent client performance
#    ‚úÖ Different data type operations
#    ‚úÖ Network latency impact
#    ‚úÖ CPU and memory utilization
#
# üìà OUTPUT:
#    - Real-time progress indicators
#    - Detailed console reports with statistics
#    - Optional JSON export for analysis
#    - Performance graphs and comparisons
#    - Error analysis and debugging info
#    - Resource utilization metrics
#
# üóÇÔ∏è TEST OPERATIONS:
#    - String operations: SET, GET, DEL, EXISTS, INCR
#    - List operations: LPUSH, LPOP, RPUSH, RPOP, LLEN
#    - Hash operations: HSET, HGET, HDEL, HLEN
#    - Set operations: SADD, SMEMBERS, SREM, SCARD
#    - Pipeline operations for bulk efficiency
#
# ‚ö†Ô∏è  SAFETY NOTES:
#    - Uses separate test database (configurable)
#    - Automatic cleanup after tests
#    - Rate limiting options to prevent overload
#    - Safe for production testing with proper limits
#    - Connection pooling for efficiency
#
# üèÜ REDIS PERFORMANCE TIPS:
#    - Use pipelining for bulk operations
#    - Monitor memory usage and fragmentation
#    - Optimize maxmemory-policy settings
#    - Use appropriate data structures for your use case
#    - Configure proper timeout values
#    - Monitor slow query log
#    - Use connection pooling
#    - Consider Redis Cluster for horizontal scaling
#
# üìÑ GENERATED REPORTS:
#    - Console output: Real-time results with detailed statistics
#    - JSON file: redis_benchmark_YYYYMMDD_HHMMSS.json with all metrics
#    - Includes: throughput, latency percentiles, error rates, memory usage
#
# üêõ TROUBLESHOOTING:
#    - "Connection refused" ‚Üí Check Redis server status and port
#    - "NOAUTH Authentication required" ‚Üí Provide username/password
#    - "OOM Out of Memory" ‚Üí Reduce test size or increase Redis memory
#    - High latency ‚Üí Check network connection and Redis configuration
#    - "Too many clients" ‚Üí Reduce concurrent client count
#    - Pipeline errors ‚Üí Reduce pipeline size or increase timeout
#
# üìö REDIS-SPECIFIC NOTES:
#    - Tests multiple Redis data types
#    - Supports Redis AUTH and ACL
#    - Compatible with Redis 6.x and 7.x
#    - Tests both single operations and pipelines
#    - Measures memory efficiency
#    - Validates persistence behavior
#
# üåê KUBERNETES SETUP:
#    # Port forward Redis service
#    kubectl port-forward svc/redis 6379:6379
#
#    # Or for Redis with authentication
#    kubectl get secret redis-secret -o jsonpath="{.data.password}" | base64 -d
#    python3 redis_benchmark.py --host localhost --port 6379 --password <password>
#
# üìû SUPPORT:
#    This tool was generated and refined by AI. For Redis performance
#    tuning, consult the official Redis documentation at redis.io/documentation.
#
################################################################################

A comprehensive Redis benchmark tool that measures performance across
multiple operations, data types, and concurrency scenarios.

Features:
- Multiple operation types (strings, lists, hashes, sets)
- Pipeline support for bulk operations
- Concurrent client simulation
- Detailed latency analysis
- Memory usage monitoring
- JSON result export

Requirements:
- Python 3.6+
- redis library
- Access to Redis server

Usage:
    python3 redis_benchmark.py --host localhost --port 6379
"""

import redis
import time
import argparse
import json
import threading
from datetime import datetime
from statistics import mean, median, pstdev
from concurrent.futures import ThreadPoolExecutor, as_completed


class RedisBenchmark:
    """
    Comprehensive Redis benchmark tool for performance testing.

    Supports multiple operation types, concurrent clients, and detailed
    performance analysis with export capabilities.
    """

    def __init__(self, host='localhost', port=6379, username=None, password=None,
                 db=0, timeout=30, max_connections=None):
        """
        Initialize Redis benchmark with connection parameters.

        Args:
            host (str): Redis server hostname
            port (int): Redis server port
            username (str): Redis username (for ACL)
            password (str): Redis password
            db (int): Redis database number
            timeout (int): Connection timeout in seconds
            max_connections (int): Maximum connections in pool
        """
        # Connection pool for better performance
        pool_kwargs = {
            'host': host,
            'port': port,
            'db': db,
            'decode_responses': True,
            'socket_timeout': timeout,
            'socket_connect_timeout': timeout,
        }

        if username:
            pool_kwargs['username'] = username
        if password:
            pool_kwargs['password'] = password
        if max_connections:
            pool_kwargs['max_connections'] = max_connections

        self.pool = redis.ConnectionPool(**pool_kwargs)
        self.r = redis.Redis(connection_pool=self.pool)

        # Thread-safe storage for results
        self._lock = threading.Lock()
        self.latencies = []
        self.errors = []
        self.results = {
            'connection': {},
            'tests': {},
            'summary': {},
            'config': {
                'host': host,
                'port': port,
                'username': username,
                'db': db,
                'timeout': timeout
            }
        }

    def check_connection(self):
        """
        Test Redis connection and gather server information.

        Returns:
            bool: True if connection successful, False otherwise
        """
        print("üîå Testing Redis connection...")

        try:
            start_time = time.time()

            # Test basic connectivity
            response = self.r.ping()
            connection_time = time.time() - start_time

            if response:
                print(f"‚úÖ Connected successfully in {connection_time:.3f}s")

                # Gather server information
                try:
                    info = self.r.info()
                    server_info = {
                        'redis_version': info.get('redis_version', 'Unknown'),
                        'used_memory_human': info.get('used_memory_human', 'Unknown'),
                        'connected_clients': info.get('connected_clients', 0),
                        'total_commands_processed': info.get('total_commands_processed', 0),
                        'keyspace_hits': info.get('keyspace_hits', 0),
                        'keyspace_misses': info.get('keyspace_misses', 0)
                    }

                    print(f"   Redis Version: {server_info['redis_version']}")
                    print(f"   Memory Usage: {server_info['used_memory_human']}")
                    print(f"   Connected Clients: {server_info['connected_clients']}")

                    self.results['connection'] = {
                        'success': True,
                        'time': connection_time,
                        'server_info': server_info
                    }
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Could not get server info: {e}")
                    self.results['connection'] = {
                        'success': True,
                        'time': connection_time,
                        'server_info': {}
                    }

                return True
            else:
                print("‚ùå Connection failed - no response to PING")
                self.results['connection'] = {'success': False, 'error': 'No PING response'}
                return False

        except redis.AuthenticationError as e:
            print(f"‚ùå Authentication failed: {e}")
            self.results['connection'] = {'success': False, 'error': f'Auth error: {e}'}
            return False
        except redis.ConnectionError as e:
            print(f"‚ùå Connection failed: {e}")
            self.results['connection'] = {'success': False, 'error': f'Connection error: {e}'}
            return False
        except Exception as e:
            print(f"‚ùå Unexpected error: {e}")
            self.results['connection'] = {'success': False, 'error': f'Unexpected error: {e}'}
            return False

    def flush_db(self):
        """Clear the test database and prepare for benchmarking."""
        try:
            self.r.flushdb()
            print("‚úÖ Test database cleared")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not flush database: {e}")

    def _record_operation(self, start_time, success=True, error=None):
        """
        Thread-safe recording of operation results.

        Args:
            start_time (float): Operation start time
            success (bool): Whether operation succeeded
            error (str): Error message if operation failed
        """
        latency = (time.perf_counter() - start_time) * 1000  # Convert to milliseconds

        with self._lock:
            if success:
                self.latencies.append(latency)
            else:
                self.errors.append(error or "Unknown error")

    def _string_operations_worker(self, num_ops, key_prefix, value, pipeline_size=None):
        """
        Worker function for string operations (SET, GET, DEL, EXISTS).

        Args:
            num_ops (int): Number of operations to perform
            key_prefix (str): Prefix for Redis keys
            value (str): Value to store
            pipeline_size (int): Pipeline size for bulk operations
        """
        r = redis.Redis(connection_pool=self.pool)

        # Pipeline mode for better performance
        if pipeline_size and pipeline_size > 1:
            pipe = r.pipeline()
            for i in range(num_ops):
                key = f"{key_prefix}:{i}"

                # Add operations to pipeline
                start = time.perf_counter()
                pipe.set(key, value)
                pipe.get(key)
                pipe.exists(key)
                pipe.delete(key)

                # Execute pipeline in batches
                if (i + 1) % pipeline_size == 0:
                    try:
                        pipe.execute()
                        self._record_operation(start, success=True)
                    except Exception as e:
                        self._record_operation(start, success=False, error=str(e))
                    pipe = r.pipeline()

            # Execute remaining operations
            if len(pipe.command_stack) > 0:
                try:
                    start = time.perf_counter()
                    pipe.execute()
                    self._record_operation(start, success=True)
                except Exception as e:
                    self._record_operation(start, success=False, error=str(e))
        else:
            # Individual operations
            for i in range(num_ops):
                key = f"{key_prefix}:{i}"

                try:
                    # SET operation
                    start = time.perf_counter()
                    r.set(key, value)
                    self._record_operation(start, success=True)

                    # GET operation
                    start = time.perf_counter()
                    r.get(key)
                    self._record_operation(start, success=True)

                    # EXISTS operation
                    start = time.perf_counter()
                    r.exists(key)
                    self._record_operation(start, success=True)

                    # DELETE operation
                    start = time.perf_counter()
                    r.delete(key)
                    self._record_operation(start, success=True)

                except Exception as e:
                    self._record_operation(start, success=False, error=str(e))

    def _list_operations_worker(self, num_ops, key_prefix, value):
        """
        Worker function for list operations (LPUSH, LPOP, RPUSH, RPOP).

        Args:
            num_ops (int): Number of operations to perform
            key_prefix (str): Prefix for Redis keys
            value (str): Value to store in lists
        """
        r = redis.Redis(connection_pool=self.pool)

        for i in range(num_ops):
            key = f"{key_prefix}:list:{i}"

            try:
                # LPUSH operation
                start = time.perf_counter()
                r.lpush(key, value)
                self._record_operation(start, success=True)

                # RPUSH operation
                start = time.perf_counter()
                r.rpush(key, value)
                self._record_operation(start, success=True)

                # LLEN operation
                start = time.perf_counter()
                r.llen(key)
                self._record_operation(start, success=True)

                # LPOP operation
                start = time.perf_counter()
                r.lpop(key)
                self._record_operation(start, success=True)

                # RPOP operation
                start = time.perf_counter()
                r.rpop(key)
                self._record_operation(start, success=True)

            except Exception as e:
                self._record_operation(start, success=False, error=str(e))

    def _hash_operations_worker(self, num_ops, key_prefix, field, value):
        """
        Worker function for hash operations (HSET, HGET, HDEL).

        Args:
            num_ops (int): Number of operations to perform
            key_prefix (str): Prefix for Redis keys
            field (str): Hash field name
            value (str): Value to store in hash
        """
        r = redis.Redis(connection_pool=self.pool)

        for i in range(num_ops):
            key = f"{key_prefix}:hash:{i}"

            try:
                # HSET operation
                start = time.perf_counter()
                r.hset(key, field, value)
                self._record_operation(start, success=True)

                # HGET operation
                start = time.perf_counter()
                r.hget(key, field)
                self._record_operation(start, success=True)

                # HLEN operation
                start = time.perf_counter()
                r.hlen(key)
                self._record_operation(start, success=True)

                # HDEL operation
                start = time.perf_counter()
                r.hdel(key, field)
                self._record_operation(start, success=True)

            except Exception as e:
                self._record_operation(start, success=False, error=str(e))

    def run_benchmark(self, operation_type='string', num_clients=50, total_requests=100000,
                     key_size=32, value_size=128, pipeline_size=None):
        """
        Run comprehensive benchmark test.

        Args:
            operation_type (str): Type of operations ('string', 'list', 'hash')
            num_clients (int): Number of concurrent clients
            total_requests (int): Total number of requests
            key_size (int): Size of Redis keys
            value_size (int): Size of values
            pipeline_size (int): Pipeline size for bulk operations
        """
        print(f"\nüèÅ Starting {operation_type.upper()} benchmark")
        print(f"   Clients: {num_clients}")
        print(f"   Total requests: {total_requests:,}")
        print(f"   Key size: {key_size} bytes")
        print(f"   Value size: {value_size} bytes")
        if pipeline_size:
            print(f"   Pipeline size: {pipeline_size}")
        print("="*60)

        # Reset counters
        self.latencies = []
        self.errors = []

        # Generate test data
        key_prefix = f"bench:{operation_type}:{key_size}"
        value = 'v' * value_size
        field = 'f' * (key_size // 2)

        ops_per_client = total_requests // num_clients
        start_time = time.time()

        # Run benchmark with thread pool
        with ThreadPoolExecutor(max_workers=num_clients) as executor:
            futures = []

            for client_id in range(num_clients):
                client_key_prefix = f"{key_prefix}:client{client_id}"

                if operation_type == 'string':
                    future = executor.submit(
                        self._string_operations_worker,
                        ops_per_client, client_key_prefix, value, pipeline_size
                    )
                elif operation_type == 'list':
                    future = executor.submit(
                        self._list_operations_worker,
                        ops_per_client, client_key_prefix, value
                    )
                elif operation_type == 'hash':
                    future = executor.submit(
                        self._hash_operations_worker,
                        ops_per_client, client_key_prefix, field, value
                    )
                else:
                    raise ValueError(f"Unknown operation type: {operation_type}")

                futures.append(future)

            # Wait for all clients to complete
            completed = 0
            for future in as_completed(futures):
                try:
                    future.result()  # This will raise any exceptions
                    completed += 1
                    if completed % 10 == 0:
                        print(f"   üìä {completed}/{num_clients} clients completed...")
                except Exception as e:
                    print(f"   ‚ùå Client error: {e}")

        end_time = time.time()
        duration = end_time - start_time

        # Calculate and store results
        self._calculate_and_store_results(operation_type, duration, total_requests)

    def _calculate_and_store_results(self, operation_type, duration, expected_requests):
        """
        Calculate comprehensive performance statistics.

        Args:
            operation_type (str): Type of operations tested
            duration (float): Test duration in seconds
            expected_requests (int): Expected number of requests
        """
        successful_ops = len(self.latencies)
        error_count = len(self.errors)
        total_ops = successful_ops + error_count

        if successful_ops > 0:
            # Calculate latency statistics
            sorted_latencies = sorted(self.latencies)
            results = {
                'operation_type': operation_type,
                'total_operations': total_ops,
                'successful_operations': successful_ops,
                'errors': error_count,
                'error_rate': (error_count / total_ops * 100) if total_ops > 0 else 0,
                'duration': duration,
                'throughput': successful_ops / duration,
                'latency_stats': {
                    'mean': mean(self.latencies),
                    'median': median(self.latencies),
                    'p95': sorted_latencies[int(len(sorted_latencies) * 0.95)],
                    'p99': sorted_latencies[int(len(sorted_latencies) * 0.99)],
                    'min': min(self.latencies),
                    'max': max(self.latencies),
                    'stdev': pstdev(self.latencies) if len(self.latencies) > 1 else 0
                }
            }
        else:
            results = {
                'operation_type': operation_type,
                'total_operations': total_ops,
                'successful_operations': 0,
                'errors': error_count,
                'error_rate': 100,
                'duration': duration,
                'throughput': 0,
                'latency_stats': {}
            }

        self.results['tests'][operation_type] = results
        self._print_results(results)

    def _print_results(self, results):
        """
        Print formatted benchmark results to console.

        Args:
            results (dict): Test results dictionary
        """
        print(f"\nüìä {results['operation_type'].upper()} Benchmark Results:")
        print("=" * 60)
        print(f"Total operations: {results['total_operations']:,}")
        print(f"Successful operations: {results['successful_operations']:,}")
        print(f"Errors: {results['errors']:,}")
        print(f"Error rate: {results['error_rate']:.2f}%")
        print(f"Duration: {results['duration']:.2f}s")
        print(f"Throughput: {results['throughput']:,.2f} ops/sec")

        if results['latency_stats']:
            stats = results['latency_stats']
            print(f"\nLatency Statistics:")
            print(f"  Mean: {stats['mean']:.2f}ms")
            print(f"  Median: {stats['median']:.2f}ms")
            print(f"  95th percentile: {stats['p95']:.2f}ms")
            print(f"  99th percentile: {stats['p99']:.2f}ms")
            print(f"  Min: {stats['min']:.2f}ms")
            print(f"  Max: {stats['max']:.2f}ms")
            print(f"  Std deviation: {stats['stdev']:.2f}ms")

        print("=" * 60)

    def generate_report(self, output_file=None):
        """
        Generate comprehensive benchmark report.

        Args:
            output_file (str): Optional JSON file to save results
        """
        # Add summary statistics
        self.results['summary'] = {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(self.results['tests']),
            'overall_success': all(
                test.get('error_rate', 100) < 50
                for test in self.results['tests'].values()
            )
        }

        # Print summary to console
        print(f"\n{'='*80}")
        print(f"üìã REDIS BENCHMARK SUMMARY REPORT")
        print(f"{'='*80}")
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Redis Server: {self.results['config']['host']}:{self.results['config']['port']}")

        if self.results['connection'].get('success'):
            server_info = self.results['connection'].get('server_info', {})
            print(f"Redis Version: {server_info.get('redis_version', 'Unknown')}")
            print(f"Memory Usage: {server_info.get('used_memory_human', 'Unknown')}")

        print(f"{'='*80}")

        # Print test results summary
        for test_name, test_results in self.results['tests'].items():
            print(f"\n{test_name.upper()} Operations:")
            print(f"  Throughput: {test_results['throughput']:,.0f} ops/sec")
            if test_results['latency_stats']:
                print(f"  Avg Latency: {test_results['latency_stats']['mean']:.2f}ms")
                print(f"  P99 Latency: {test_results['latency_stats']['p99']:.2f}ms")
            print(f"  Error Rate: {test_results['error_rate']:.2f}%")

        print(f"\n{'='*80}")

        # Save to JSON file if requested
        if output_file:
            try:
                with open(output_file, 'w') as f:
                    json.dump(self.results, f, indent=2, default=str)
                print(f"üìÑ Detailed results saved to: {output_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Could not save results to {output_file}: {e}")

    def cleanup(self):
        """Clean up test data and close connections."""
        try:
            self.flush_db()
            # Close connection pool
            if hasattr(self.pool, 'disconnect'):
                self.pool.disconnect()
            print("‚úÖ Cleanup completed")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning during cleanup: {e}")


def main():
    """Main function to run Redis benchmark with command line arguments."""
    parser = argparse.ArgumentParser(
        description='Comprehensive Redis Benchmark Tool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic benchmark
  python3 redis_benchmark.py --host localhost --port 6379

  # High-throughput test
  python3 redis_benchmark.py --host localhost --port 6379 \\
                           --clients 200 --requests 1000000

  # With authentication
  python3 redis_benchmark.py --host redis.example.com --port 6379 \\
                           --username myuser --password mypass

  # Pipeline mode
  python3 redis_benchmark.py --host localhost --port 6379 \\
                           --pipeline --pipeline-size 100

  # Export to JSON
  python3 redis_benchmark.py --host localhost --port 6379 \\
                           --output results.json
        """
    )

    # Connection parameters
    parser.add_argument('--host', default='localhost', help='Redis host (default: localhost)')
    parser.add_argument('--port', type=int, default=6379, help='Redis port (default: 6379)')
    parser.add_argument('--username', help='Redis username (for ACL authentication)')
    parser.add_argument('--password', help='Redis password')
    parser.add_argument('--db', type=int, default=0, help='Redis database number (default: 0)')

    # Benchmark parameters
    parser.add_argument('--clients', type=int, default=50,
                       help='Number of concurrent clients (default: 50)')
    parser.add_argument('--requests', type=int, default=100000,
                       help='Total number of requests (default: 100000)')
    parser.add_argument('--key-size', type=int, default=32,
                       help='Size of keys in bytes (default: 32)')
    parser.add_argument('--value-size', type=int, default=128,
                       help='Size of values in bytes (default: 128)')

    # Advanced options
    parser.add_argument('--operations', default='string,list,hash',
                       help='Comma-separated list of operations to test (default: string,list,hash)')
    parser.add_argument('--pipeline', action='store_true',
                       help='Enable pipeline mode for bulk operations')
    parser.add_argument('--pipeline-size', type=int, default=10,
                       help='Pipeline size for bulk operations (default: 10)')
    parser.add_argument('--timeout', type=int, default=30,
                       help='Connection timeout in seconds (default: 30)')
    parser.add_argument('--max-connections', type=int,
                       help='Maximum connections in pool')

    # Output options
    parser.add_argument('--output', help='Output file for JSON results')
    parser.add_argument('--no-cleanup', action='store_true',
                       help='Skip database cleanup (useful for debugging)')

    args = parser.parse_args()

    # Validate arguments
    if args.clients <= 0 or args.requests <= 0:
        print("‚ùå Error: Client count and request count must be positive")
        return 1

    if args.key_size < 1 or args.value_size < 1:
        print("‚ùå Error: Key size and value size must be at least 1 byte")
        return 1

    # Parse operations list
    operations = [op.strip().lower() for op in args.operations.split(',')]
    valid_operations = ['string', 'list', 'hash']

    for op in operations:
        if op not in valid_operations:
            print(f"‚ùå Error: Invalid operation '{op}'. Valid options: {', '.join(valid_operations)}")
            return 1

    # Check redis library
    try:
        import redis
    except ImportError:
        print("‚ùå Error: redis library is required")
        print("Install with: pip install redis")
        return 1

    # Initialize benchmark
    print("üöÄ Starting Redis Benchmark Suite")
    print("="*70)

    benchmark = RedisBenchmark(
        host=args.host,
        port=args.port,
        username=args.username,
        password=args.password,
        db=args.db,
        timeout=args.timeout,
        max_connections=args.max_connections
    )

    try:
        # Test connection
        if not benchmark.check_connection():
            print("‚ùå Cannot proceed without Redis connection")
            return 1

        # Prepare test environment
        if not args.no_cleanup:
            benchmark.flush_db()

        # Run benchmarks for each operation type
        pipeline_size = args.pipeline_size if args.pipeline else None

        for operation in operations:
            benchmark.run_benchmark(
                operation_type=operation,
                num_clients=args.clients,
                total_requests=args.requests,
                key_size=args.key_size,
                value_size=args.value_size,
                pipeline_size=pipeline_size
            )

        # Generate final report
        benchmark.generate_report(args.output)

    except KeyboardInterrupt:
        print("\nüö´ Benchmark interrupted by user")
        return 1
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        return 1
    finally:
        # Cleanup
        if not args.no_cleanup:
            benchmark.cleanup()

    print("\n‚úÖ Redis benchmark completed successfully!")
    return 0


if __name__ == "__main__":
    exit(main())