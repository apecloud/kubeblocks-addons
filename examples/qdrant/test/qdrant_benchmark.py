#!/usr/bin/env python3
"""
################################################################################
#                       Qdrant Vector Database Benchmark Test Suite
################################################################################
#
# 🤖 This benchmark test was generated by GPT (Claude Sonnet 4) to provide
#    comprehensive Qdrant vector database performance testing.
#
# 📋 DESCRIPTION:
#    A professional-grade benchmark testing tool that measures Qdrant
#    performance across multiple vector operations:
#    - Vector insertion (batch and single)
#    - Vector similarity search with various distance metrics
#    - Filtered search with payload conditions
#    - Concurrent search performance testing
#    - Vector updates and deletions
#    - Collection management and optimization
#
# 🔧 PREREQUISITES:
#    - Python 3.6 or higher
#    - Qdrant server (1.0+ recommended)
#    - qdrant-client Python library (pip install qdrant-client)
#    - numpy library (pip install numpy)
#    - Access to Qdrant server (local or remote)
#
# 📦 INSTALLATION:
#    pip install qdrant-client numpy
#
#    # Start Qdrant server locally:
#    docker run -p 6333:6333 qdrant/qdrant:latest
#
#    # Or install Qdrant directly:
#    # See: https://qdrant.tech/documentation/quick-start/

# 🚀 BASIC USAGE:
#    python3 qdrant_benchmark.py --host localhost --port 6333
#
# 🎯 ADVANCED USAGE EXAMPLES:
#
#    # Quick test with small dataset
#    python3 qdrant_benchmark.py --host localhost --port 6333 \
#                               --num-vectors 1000 --vector-size 64 --search-queries 100
#
#    # High-performance test with large dataset
#    python3 qdrant_benchmark.py --host remote-qdrant --port 6333 \
#                               --num-vectors 100000 --vector-size 512 --search-queries 5000 \
#                               --workers 20 --distance Cosine
#
#    # Test different vector dimensions and distance metrics
#    python3 qdrant_benchmark.py --host localhost --vector-size 256 \
#                               --distance Euclidean --no-payload
#
# 📊 WHAT IT MEASURES:
#    ✅ Connection performance and stability
#    ✅ Vector insertion throughput (batch operations)
#    ✅ Search latency and accuracy across distance metrics
#    ✅ Filtered search performance with payload conditions
#    ✅ Concurrent search handling and scaling
#    ✅ Vector update and deletion performance
#    ✅ Collection management efficiency
#    ✅ Memory usage and storage optimization
#
# 📈 OUTPUT:
#    - Real-time progress with emoji indicators
#    - Comprehensive console report with key metrics
#    - JSON file export with detailed results for analysis
#    - Performance comparison across different operations
#    - Vector-specific metrics (indexing time, search accuracy, etc.)
#
# 🗂️ TEST COLLECTION:
#    The benchmark automatically creates a temporary test collection
#    'benchmark_collection' with configurable parameters. All test data
#    is cleaned up automatically after completion.
#
# ⚠️  SAFETY NOTES:
#    - Only creates temporary test collections, doesn't affect existing data
#    - Test collection is automatically cleaned up after benchmark
#    - Safe to run on production servers (creates isolated test environment)
#    - Uses efficient batch operations to minimize server load
#
# 🏆 QDRANT PERFORMANCE TIPS:
#    - Choose appropriate vector dimensions for your use case
#    - Use batch insertions for better throughput
#    - Configure indexing parameters based on search requirements
#    - Monitor memory usage with large vector datasets
#    - Use payload filtering judiciously to maintain performance
#    - Consider segment optimization for production workloads
#
# 📄 GENERATED REPORTS:
#    - Console output: Real-time results with summary statistics
#    - JSON file: qdrant_benchmark_YYYYMMDD_HHMMSS.json with detailed metrics
#    - Metrics include: insertion rate, search latency, accuracy, error rates
#
# 🐛 TROUBLESHOOTING:
#    - "qdrant-client not found" → Install with: pip install qdrant-client
#    - "numpy not found" → Install with: pip install numpy
#    - Connection failures → Check Qdrant server status and network
#    - "Collection already exists" → The benchmark handles cleanup automatically
#    - Performance issues → Check vector dimensions and batch sizes
#    - Memory errors → Reduce num-vectors or vector-size parameters
#    - "search method deprecated" → Update to latest qdrant-client version
#    - API compatibility → Requires qdrant-client 1.0+ for query_points support
#
# 📚 QDRANT-SPECIFIC NOTES:
#    - Supports all Qdrant distance metrics (Cosine, Euclidean, Dot)
#    - Tests both with and without payload data
#    - Automatically handles collection configuration
#    - Compatible with Qdrant 1.0+ API (uses query_points instead of deprecated search)
#    - Optimized for vector similarity search workloads
#    - Uses modern Qdrant client API methods
#
# 📞 SUPPORT:
#    This tool was generated by AI and is provided as-is. For Qdrant
#    performance tuning, consult the official Qdrant documentation
#    at https://qdrant.tech/documentation/
#
################################################################################

A comprehensive benchmark testing tool for Qdrant vector database that measures:
- Vector insertion and retrieval performance
- Search accuracy and latency
- Concurrent operation handling
- Filtered search performance
- Collection management efficiency

Requirements:
- Python 3.6+
- Qdrant server running
- qdrant-client and numpy packages

Usage:
    python3 qdrant_benchmark.py --host localhost --port 6333
"""

import argparse
import json
import logging
import os
import statistics
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

# Check for numpy
try:
    import numpy as np
except ImportError:
    print("❌ Error: numpy is required but not installed")
    print("Please install it with: pip install numpy")
    sys.exit(1)

# Check for qdrant-client
try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    from qdrant_client.http.models import Distance, VectorParams, PointStruct
except ImportError:
    print("❌ Error: qdrant-client is required but not installed")
    print("Please install it with: pip install qdrant-client")
    print("\nAlternatively, install both dependencies with:")
    print("pip install qdrant-client numpy")
    sys.exit(1)


@dataclass
class BenchmarkConfig:
    """Configuration class for benchmark parameters"""
    host: str = "localhost"
    port: int = 6333
    collection_name: str = "benchmark_collection"
    vector_size: int = 128
    num_vectors: int = 10000
    batch_size: int = 100
    search_queries: int = 1000
    top_k: int = 10
    concurrent_workers: int = 10
    distance_metric: str = "Cosine"  # Cosine, Euclidean, Dot
    with_payload: bool = True
    cleanup_after: bool = True


@dataclass
class BenchmarkResults:
    """Data class to store benchmark results"""
    operation: str
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    ops_per_second: float
    total_operations: int
    success_rate: float
    percentile_95: float
    percentile_99: float


class QdrantBenchmark:
    """
    Comprehensive Qdrant benchmark class following MySQL benchmark patterns.

    This class provides methods to benchmark vector database operations with
    proper error handling, progress reporting, and results analysis.
    """

    def __init__(self, config: BenchmarkConfig):
        """
        Initialize the benchmark with configuration.

        Args:
            config: BenchmarkConfig object containing all benchmark parameters
        """
        self.config = config
        self.client = None
        self.results = {}

        # Setup clean logging configuration
        self._setup_logging()
        self.logger = logging.getLogger(__name__)

    def _setup_logging(self):
        """Configure logging to reduce noise from HTTP libraries"""
        # Configure root logger
        logging.basicConfig(
            level=logging.INFO,
            format='%(message)s',  # Simplified format for cleaner output
            force=True  # Override any existing configuration
        )

        # Suppress verbose logs from HTTP libraries
        noisy_loggers = [
            'qdrant_client',
            'urllib3',
            'httpx',
            'httpcore',
            'requests',
            'urllib3.connectionpool'
        ]

        for logger_name in noisy_loggers:
            logging.getLogger(logger_name).setLevel(logging.WARNING)

    def check_dependencies(self) -> bool:
        """Check if required dependencies are installed"""
        print("🔍 Checking dependencies...")

        # Check numpy
        try:
            import numpy as np
            print(f"✅ numpy: {np.__version__}")
        except ImportError:
            print("❌ Error: numpy is required for vector operations")
            print("Installation: pip install numpy")
            return False

        # Check qdrant-client
        try:
            import qdrant_client
            print(f"✅ qdrant-client: {qdrant_client.__version__}")
        except ImportError:
            print("❌ Error: qdrant-client is required for Qdrant connections")
            print("Installation: pip install qdrant-client")
            return False
        except AttributeError:
            # Some versions don't have __version__
            print("✅ qdrant-client: Available")

        return True

    def check_connection(self) -> bool:
        """Test basic connectivity to Qdrant server"""
        print("🔌 Testing Qdrant connection...")

        start_time = time.time()
        try:
            self.client = QdrantClient(
                host=self.config.host,
                port=self.config.port,
                timeout=30
            )

            # Test connection by getting collections
            collections = self.client.get_collections()
            connection_time = time.time() - start_time

            print(f"✅ Connected successfully in {connection_time:.3f}s")
            print(f"   Host: {self.config.host}:{self.config.port}")
            print(f"   Collections found: {len(collections.collections)}")

            # Get Qdrant info if available
            try:
                cluster_info = self.client.get_cluster_info()
                if hasattr(cluster_info, 'peer_id'):
                    print(f"   Peer ID: {cluster_info.peer_id}")
            except:
                pass  # Not all Qdrant versions support this

            self.results['connection'] = {
                'success': True,
                'time': connection_time,
                'host': f"{self.config.host}:{self.config.port}",
                'collections_count': len(collections.collections)
            }
            return True

        except Exception as e:
            print(f"❌ Connection failed: {e}")

            # Provide helpful error messages
            if 'Connection refused' in str(e):
                print("\n💡 SOLUTION SUGGESTIONS:")
                print("   - Check if Qdrant server is running")
                print("   - Verify host and port are correct")
                print("   - Try: docker run -p 6333:6333 qdrant/qdrant:latest")
            elif 'timeout' in str(e).lower():
                print("\n💡 SOLUTION SUGGESTIONS:")
                print("   - Check network connectivity")
                print("   - Verify firewall settings")
                print("   - Increase timeout if server is slow")

            self.results['connection'] = {
                'success': False,
                'error': str(e)
            }
            return False

    def setup_test_environment(self) -> bool:
        """Create and configure test collection"""
        print("🔧 Setting up test environment...")

        try:
            # Delete collection if it exists
            try:
                self.client.delete_collection(self.config.collection_name)
                print(f"🗑️  Deleted existing collection: {self.config.collection_name}")
            except:
                pass  # Collection doesn't exist, which is fine

            # Create new collection with specified parameters
            distance_map = {
                "Cosine": Distance.COSINE,
                "Euclidean": Distance.EUCLID,
                "Dot": Distance.DOT
            }

            if self.config.distance_metric not in distance_map:
                print(f"❌ Invalid distance metric: {self.config.distance_metric}")
                return False

            self.client.create_collection(
                collection_name=self.config.collection_name,
                vectors_config=VectorParams(
                    size=self.config.vector_size,
                    distance=distance_map[self.config.distance_metric]
                )
            )

            print(f"✅ Created collection '{self.config.collection_name}' with:")
            print(f"   - Vector size: {self.config.vector_size}")
            print(f"   - Distance metric: {self.config.distance_metric}")
            print(f"   - Payload enabled: {self.config.with_payload}")

            return True

        except Exception as e:
            print(f"❌ Failed to setup test environment: {e}")
            return False

    def generate_random_vector(self) -> List[float]:
        """Generate a random vector of specified size"""
        return np.random.uniform(-1, 1, self.config.vector_size).tolist()

    def generate_sample_payload(self, point_id: int) -> Dict[str, Any]:
        """Generate sample payload data for a vector point"""
        categories = ["electronics", "books", "clothing", "home", "sports"]
        return {
            "id": point_id,
            "category": categories[point_id % len(categories)],
            "price": round(np.random.uniform(10, 1000), 2),
            "rating": round(np.random.uniform(1, 5), 1),
            "in_stock": bool(point_id % 2),
            "tags": [f"tag_{i}" for i in range(point_id % 5)],
            "metadata": {
                "created_at": int(time.time()),
                "source": "benchmark"
            }
        }

    def _calculate_results(self, operation: str, operation_times: List[float],
                          total_time: float, total_operations: int) -> BenchmarkResults:
        """Calculate benchmark statistics from operation times"""
        if not operation_times:
            return BenchmarkResults(
                operation=operation,
                total_time=total_time,
                avg_time=0,
                min_time=0,
                max_time=0,
                ops_per_second=0,
                total_operations=0,
                success_rate=0,
                percentile_95=0,
                percentile_99=0
            )

        return BenchmarkResults(
            operation=operation,
            total_time=total_time,
            avg_time=statistics.mean(operation_times),
            min_time=min(operation_times),
            max_time=max(operation_times),
            ops_per_second=total_operations / total_time if total_time > 0 else 0,
            total_operations=total_operations,
            success_rate=len(operation_times) / total_operations * 100 if total_operations > 0 else 0,
            percentile_95=np.percentile(operation_times, 95),
            percentile_99=np.percentile(operation_times, 99)
        )

    def run_benchmark_suite(self) -> Dict[str, BenchmarkResults]:
        """Run comprehensive Qdrant benchmark suite"""
        print(f"🚀 Starting Qdrant Vector Database Benchmark Suite")
        print(f"   Collection: {self.config.collection_name}")
        print(f"   Vector size: {self.config.vector_size}")
        print(f"   Num vectors: {self.config.num_vectors:,}")
        print(f"   Distance metric: {self.config.distance_metric}")
        print(f"   Search queries: {self.config.search_queries:,}")
        print("="*60)

        benchmark_results = {}

        # 1. Batch insertion benchmark
        print(f"\n📊 Vector Batch Insertion Benchmark")
        print("-" * 40)
        result = self._run_batch_insert_benchmark()
        if result:
            benchmark_results['batch_insert'] = result
            print(f"   ✅ Inserted {result.total_operations:,} vectors")
            print(f"   📈 Rate: {result.ops_per_second:.2f} vectors/sec")
            print(f"   ⏱️  Avg time: {result.avg_time*1000:.2f}ms per batch")

        # 2. Search benchmark
        print(f"\n📊 Vector Search Benchmark")
        print("-" * 40)
        result = self._run_search_benchmark()
        if result:
            benchmark_results['search'] = result
            print(f"   ✅ Completed {result.total_operations:,} searches")
            print(f"   📈 Rate: {result.ops_per_second:.2f} searches/sec")
            print(f"   ⏱️  Avg latency: {result.avg_time*1000:.2f}ms")
            print(f"   📊 95th percentile: {result.percentile_95*1000:.2f}ms")

        # 3. Filtered search benchmark
        if self.config.with_payload:
            print(f"\n📊 Filtered Search Benchmark")
            print("-" * 40)
            result = self._run_filtered_search_benchmark()
            if result:
                benchmark_results['filtered_search'] = result
                print(f"   ✅ Completed {result.total_operations:,} filtered searches")
                print(f"   📈 Rate: {result.ops_per_second:.2f} searches/sec")
                print(f"   ⏱️  Avg latency: {result.avg_time*1000:.2f}ms")

        # 4. Concurrent search benchmark
        print(f"\n📊 Concurrent Search Benchmark")
        print("-" * 40)
        result = self._run_concurrent_search_benchmark()
        if result:
            benchmark_results['concurrent_search'] = result
            print(f"   ✅ Completed {result.total_operations:,} concurrent searches")
            print(f"   📈 Rate: {result.ops_per_second:.2f} searches/sec")
            print(f"   👥 Workers: {self.config.concurrent_workers}")

        # 5. Update benchmark
        print(f"\n📊 Vector Update Benchmark")
        print("-" * 40)
        result = self._run_update_benchmark()
        if result:
            benchmark_results['update'] = result
            print(f"   ✅ Updated {result.total_operations:,} vectors")
            print(f"   📈 Rate: {result.ops_per_second:.2f} updates/sec")

        # 6. Delete benchmark
        print(f"\n📊 Vector Deletion Benchmark")
        print("-" * 40)
        result = self._run_delete_benchmark()
        if result:
            benchmark_results['delete'] = result
            print(f"   ✅ Deleted {result.total_operations:,} vectors")
            print(f"   📈 Rate: {result.ops_per_second:.2f} deletions/sec")

        self.results['benchmark_suite'] = benchmark_results
        return benchmark_results

    def _run_batch_insert_benchmark(self) -> Optional[BenchmarkResults]:
        """Run batch vector insertion benchmark"""
        try:
            operation_times = []
            total_inserted = 0
            num_batches = self.config.num_vectors // self.config.batch_size

            start_time = time.time()

            for batch_idx in range(num_batches):
                batch_start = time.time()

                # Generate batch of points
                points = []
                for i in range(self.config.batch_size):
                    point_id = batch_idx * self.config.batch_size + i
                    vector = self.generate_random_vector()
                    payload = self.generate_sample_payload(point_id) if self.config.with_payload else None

                    points.append(PointStruct(
                        id=point_id,
                        vector=vector,
                        payload=payload
                    ))

                # Insert batch
                self.client.upsert(
                    collection_name=self.config.collection_name,
                    points=points
                )

                batch_time = time.time() - batch_start
                operation_times.append(batch_time)
                total_inserted += len(points)

                if (batch_idx + 1) % 10 == 0:
                    print(f"   🔄 Inserted batch {batch_idx + 1}/{num_batches}")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Batch Insert",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=total_inserted
            )

        except Exception as e:
            print(f"   ❌ Batch insert benchmark failed: {e}")
            return None

    def _run_search_benchmark(self) -> Optional[BenchmarkResults]:
        """Run vector search benchmark"""
        try:
            operation_times = []
            successful_searches = 0

            start_time = time.time()

            for i in range(self.config.search_queries):
                query_start = time.time()
                query_vector = self.generate_random_vector()

                results = self.client.query_points(
                    collection_name=self.config.collection_name,
                    query=query_vector,
                    limit=self.config.top_k
                )

                query_time = time.time() - query_start
                operation_times.append(query_time)
                successful_searches += 1

                if (i + 1) % 100 == 0:
                    print(f"   🔄 Completed {i + 1}/{self.config.search_queries} searches")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Vector Search",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=successful_searches
            )

        except Exception as e:
            print(f"   ❌ Search benchmark failed: {e}")
            return None

    def _run_filtered_search_benchmark(self) -> Optional[BenchmarkResults]:
        """Run filtered search benchmark"""
        try:
            operation_times = []
            successful_searches = 0
            categories = ["electronics", "books", "clothing", "home", "sports"]

            start_time = time.time()
            num_queries = self.config.search_queries // 2  # Fewer queries for filtered search

            for i in range(num_queries):
                query_start = time.time()
                query_vector = self.generate_random_vector()
                category_filter = categories[i % len(categories)]

                results = self.client.query_points(
                    collection_name=self.config.collection_name,
                    query=query_vector,
                    query_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="category",
                                match=models.MatchValue(value=category_filter)
                            )
                        ]
                    ),
                    limit=self.config.top_k
                )

                query_time = time.time() - query_start
                operation_times.append(query_time)
                successful_searches += 1

                if (i + 1) % 50 == 0:
                    print(f"   🔄 Completed {i + 1}/{num_queries} filtered searches")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Filtered Search",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=successful_searches
            )

        except Exception as e:
            print(f"   ❌ Filtered search benchmark failed: {e}")
            return None

    def _run_concurrent_search_benchmark(self) -> Optional[BenchmarkResults]:
        """Run concurrent search benchmark"""
        try:
            operation_times = []
            successful_searches = 0

            def perform_search(_) -> Optional[float]:
                """Single search operation for thread pool"""
                query_start = time.time()
                query_vector = self.generate_random_vector()

                try:
                    results = self.client.query_points(
                        collection_name=self.config.collection_name,
                        query=query_vector,
                        limit=self.config.top_k
                    )
                    return time.time() - query_start
                except Exception:
                    return None

            start_time = time.time()

            # Use ThreadPoolExecutor for concurrent searches
            with ThreadPoolExecutor(max_workers=self.config.concurrent_workers) as executor:
                futures = [executor.submit(perform_search, i) for i in range(self.config.search_queries)]

                for i, future in enumerate(as_completed(futures)):
                    result = future.result()
                    if result is not None:
                        operation_times.append(result)
                        successful_searches += 1

                    if (i + 1) % 100 == 0:
                        print(f"   🔄 Completed {i + 1}/{self.config.search_queries} concurrent searches")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Concurrent Search",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=successful_searches
            )

        except Exception as e:
            print(f"   ❌ Concurrent search benchmark failed: {e}")
            return None

    def _run_update_benchmark(self) -> Optional[BenchmarkResults]:
        """Run vector update benchmark"""
        try:
            operation_times = []
            successful_updates = 0
            num_updates = min(1000, self.config.num_vectors // 10)  # Update 10% of vectors

            start_time = time.time()

            for i in range(num_updates):
                update_start = time.time()
                point_id = i % self.config.num_vectors
                new_vector = self.generate_random_vector()
                new_payload = self.generate_sample_payload(point_id) if self.config.with_payload else None

                self.client.upsert(
                    collection_name=self.config.collection_name,
                    points=[PointStruct(
                        id=point_id,
                        vector=new_vector,
                        payload=new_payload
                    )]
                )

                update_time = time.time() - update_start
                operation_times.append(update_time)
                successful_updates += 1

                if (i + 1) % 100 == 0:
                    print(f"   🔄 Updated {i + 1}/{num_updates} vectors")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Vector Update",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=successful_updates
            )

        except Exception as e:
            print(f"   ❌ Update benchmark failed: {e}")
            return None

    def _run_delete_benchmark(self) -> Optional[BenchmarkResults]:
        """Run vector deletion benchmark"""
        try:
            operation_times = []
            successful_deletions = 0
            num_deletions = min(500, self.config.num_vectors // 20)  # Delete 5% of vectors

            start_time = time.time()

            for i in range(num_deletions):
                delete_start = time.time()
                point_id = self.config.num_vectors - 1 - i  # Delete from end

                self.client.delete(
                    collection_name=self.config.collection_name,
                    points_selector=models.PointIdsList(
                        points=[point_id]
                    )
                )

                delete_time = time.time() - delete_start
                operation_times.append(delete_time)
                successful_deletions += 1

                if (i + 1) % 100 == 0:
                    print(f"   🔄 Deleted {i + 1}/{num_deletions} vectors")

            total_time = time.time() - start_time

            return self._calculate_results(
                operation="Vector Deletion",
                operation_times=operation_times,
                total_time=total_time,
                total_operations=successful_deletions
            )

        except Exception as e:
            print(f"   ❌ Delete benchmark failed: {e}")
            return None

    def get_collection_info(self) -> Dict[str, Any]:
        """Get detailed information about the test collection"""
        try:
            info = self.client.get_collection(self.config.collection_name)
            return {
                "status": info.status,
                "vectors_count": info.vectors_count,
                "indexed_vectors_count": info.indexed_vectors_count,
                "points_count": info.points_count,
                "segments_count": info.segments_count,
                "config": {
                    "vector_size": info.config.params.vectors.size,
                    "distance": info.config.params.vectors.distance.name
                }
            }
        except Exception as e:
            print(f"⚠️  Warning: Could not get collection info: {e}")
            return {}

    def cleanup(self) -> None:
        """Clean up test collection"""
        print("🧹 Cleaning up test environment...")

        try:
            self.client.delete_collection(self.config.collection_name)
            print("✅ Test collection cleaned up successfully")
        except Exception as e:
            print(f"⚠️  Warning: Could not clean up test collection: {e}")

    def generate_report(self) -> None:
        """Generate a comprehensive benchmark report"""
        print("\n" + "="*80)
        print("📋 QDRANT VECTOR DATABASE BENCHMARK REPORT")
        print("="*80)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Host: {self.config.host}:{self.config.port}")
        print(f"Collection: {self.config.collection_name}")
        print("-"*80)

        # Connection info
        if 'connection' in self.results:
            conn = self.results['connection']
            print(f"Connection Test: {'✅ PASS' if conn['success'] else '❌ FAIL'}")
            if conn['success']:
                print(f"  Connection Time: {conn['time']:.3f}s")
                print(f"  Server: {conn.get('host', 'Unknown')}")

        # Collection info
        collection_info = self.get_collection_info()
        if collection_info:
            print(f"\nCollection Information:")
            print(f"  Status: {collection_info.get('status', 'N/A')}")
            print(f"  Points Count: {collection_info.get('points_count', 'N/A'):,}")
            print(f"  Vector Size: {collection_info.get('config', {}).get('vector_size', 'N/A')}")
            print(f"  Distance Metric: {collection_info.get('config', {}).get('distance', 'N/A')}")

        # Benchmark suite results
        if 'benchmark_suite' in self.results:
            print(f"\n📊 BENCHMARK SUITE RESULTS")
            print("-" * 80)
            print(f"{'Operation':<20} {'Total Time':<12} {'Avg Time':<12} {'Ops/Sec':<12} {'Success %':<12} {'95th %ile':<12}")
            print("-" * 80)

            for operation_name, result in self.results['benchmark_suite'].items():
                print(f"{result.operation:<20} "
                      f"{result.total_time:<12.3f} "
                      f"{result.avg_time*1000:<12.3f} "
                      f"{result.ops_per_second:<12.2f} "
                      f"{result.success_rate:<12.1f} "
                      f"{result.percentile_95*1000:<12.3f}")

            print("\nNotes:")
            print("- Times in seconds (except Avg Time and 95th percentile in milliseconds)")
            print("- Ops/Sec = Operations per second")
            print("- Success % = Percentage of successful operations")

        print("\n" + "="*80)

        # Save detailed results to JSON file
        report_file = f"qdrant_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            export_data = {
                "benchmark_config": asdict(self.config),
                "collection_info": collection_info,
                "results": self.results,
                "timestamp": time.time(),
                "summary": {
                    "total_benchmarks": len(self.results.get('benchmark_suite', {})),
                    "total_operations": sum(r.total_operations for r in self.results.get('benchmark_suite', {}).values())
                }
            }

            with open(report_file, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
            print(f"📄 Detailed results saved to: {report_file}")
        except Exception as e:
            print(f"⚠️  Warning: Could not save detailed results: {e}")

    def run_full_benchmark(self) -> bool:
        """Run the complete benchmark suite"""
        start_time = time.time()

        # Check dependencies
        if not self.check_dependencies():
            return False

        # Check connection
        if not self.check_connection():
            print("❌ Cannot proceed without Qdrant connection")
            return False

        # Setup test environment
        if not self.setup_test_environment():
            print("❌ Cannot proceed without test environment")
            return False

        try:
            # Run benchmark suite
            self.run_benchmark_suite()

        except KeyboardInterrupt:
            print("\n⚠️  Benchmark interrupted by user")
        except Exception as e:
            print(f"\n❌ Unexpected error during benchmark: {e}")
        finally:
            # Always cleanup
            if self.config.cleanup_after:
                self.cleanup()

        total_time = time.time() - start_time
        print(f"\n⏱️  Total benchmark time: {total_time:.1f} seconds")

        # Generate report
        self.generate_report()
        return True

    def _enable_verbose_logging(self):
        """Enable verbose logging for debugging purposes"""
        logging.getLogger('qdrant_client').setLevel(logging.INFO)
        logging.getLogger('urllib3').setLevel(logging.INFO)
        logging.getLogger('httpx').setLevel(logging.INFO)
        logging.getLogger('httpcore').setLevel(logging.INFO)
        print("🔍 Verbose logging enabled - HTTP requests will be shown")


def main():
    parser = argparse.ArgumentParser(
        description='Qdrant Vector Database Benchmark Test Suite',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic benchmark
  python3 qdrant_benchmark.py --host localhost --port 6333

  # Custom configuration
  python3 qdrant_benchmark.py --host remote-server --port 6333 \\
                             --num-vectors 50000 --vector-size 256 \\
                             --search-queries 2000 --distance Euclidean
        """
    )

    parser.add_argument('--host', default='localhost', help='Qdrant host (default: localhost)')
    parser.add_argument('--port', type=int, default=6333, help='Qdrant port (default: 6333)')
    parser.add_argument('--collection', default='benchmark_collection', help='Collection name')
    parser.add_argument('--vector-size', type=int, default=128, help='Vector dimension (default: 128)')
    parser.add_argument('--num-vectors', type=int, default=10000, help='Number of vectors to insert (default: 10000)')
    parser.add_argument('--batch-size', type=int, default=100, help='Batch size for insertions (default: 100)')
    parser.add_argument('--search-queries', type=int, default=1000, help='Number of search queries (default: 1000)')
    parser.add_argument('--top-k', type=int, default=10, help='Number of results to return per search (default: 10)')
    parser.add_argument('--workers', type=int, default=10, help='Concurrent workers for parallel tests (default: 10)')
    parser.add_argument('--distance', choices=['Cosine', 'Euclidean', 'Dot'], default='Cosine',
                       help='Distance metric (default: Cosine)')
    parser.add_argument('--no-payload', action='store_true', help='Disable payload in vectors')
    parser.add_argument('--no-cleanup', action='store_true', help="Don't cleanup collection after benchmark")
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging (shows HTTP requests)')

    args = parser.parse_args()

    # Validate arguments
    if args.vector_size <= 0:
        print("❌ Error: Vector size must be positive")
        sys.exit(1)

    if args.num_vectors <= 0:
        print("❌ Error: Number of vectors must be positive")
        sys.exit(1)

    if args.batch_size <= 0:
        print("❌ Error: Batch size must be positive")
        sys.exit(1)

    # Create configuration
    config = BenchmarkConfig(
        host=args.host,
        port=args.port,
        collection_name=args.collection,
        vector_size=args.vector_size,
        num_vectors=args.num_vectors,
        batch_size=args.batch_size,
        search_queries=args.search_queries,
        top_k=args.top_k,
        concurrent_workers=args.workers,
        distance_metric=args.distance,
        with_payload=not args.no_payload,
        cleanup_after=not args.no_cleanup
    )

    # Run benchmark
    benchmark = QdrantBenchmark(config)

    # Enable verbose logging if requested
    if args.verbose:
        benchmark._enable_verbose_logging()

    success = benchmark.run_full_benchmark()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
