{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be881a6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Minimal dependency RAG with DeepSeek and Qdrant\n",
    "\n",
    "In the rapidly advancing field of AI, Large Language Models have made significant strides in understanding and generating human-like text. To improve their factual accuracy, these models significantly benefit from an integration with external knowledge sources.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a framework that combines LLMs with real-time retrieval of relevant information, ensuring more accurate and contextually relevant outputs.\n",
    "\n",
    "In this example, we'll showcase an implementation using the latest [DeepSeek-V3](https://www.deepseek.com) model. It leads the way among open-source models and competes with the best closed-source models worldwide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb044259",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's start setting up all the pieces to implement the pipeline. We'll try to do this with minimal dependencies. \n",
    "\n",
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9f81b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:32.977456Z",
     "start_time": "2023-09-27T10:06:30.203757Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install \"qdrant-client[fastembed]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4382a",
   "metadata": {},
   "source": [
    "[Qdrant](https://qdrant.tech) will act as a knowledge base providing the context information for the prompts we'll be sending to the LLM.\n",
    "\n",
    "You can get a free-forever Qdrant cloud instance at http://cloud.qdrant.io. Learn about setting up your instance from the [Quickstart](https://qdrant.tech/documentation/quickstart-cloud/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f4456c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:34.283299Z",
     "start_time": "2023-09-27T10:06:32.980517Z"
    }
   },
   "outputs": [],
   "source": [
    "QDRANT_URL = \"http://localhost:6333\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c7a21",
   "metadata": {},
   "source": [
    "### Instantiating Qdrant Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd8966b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.242783Z",
     "start_time": "2023-09-27T10:06:34.289290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanshan/Documents/projects/goes/src/apecloud/kubeblocks-addons/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/b4/mt0wv8hn2j52z9tyx6j33yrw0000gn/T/ipykernel_45641/2413650276.py:3: UserWarning: Failed to obtain server version. Unable to check client-server compatibility. Set check_compatibility=False to skip version check.\n",
      "  client = QdrantClient(url=QDRANT_URL)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f54205",
   "metadata": {},
   "source": [
    "### Building the knowledge base\n",
    "\n",
    "Qdrant will use vector embeddings of our facts to enrich the original prompt with some context. Thus, we need to store the vector embeddings and the facts used to generate them.\n",
    "\n",
    "We'll be using the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) model via [FastEmbed](https://github.com/qdrant/fastembed/) - A lightweight, fast, Python library for embeddings generation.\n",
    "\n",
    "The Qdrant client provides a handy integration with FastEmbed that makes building a knowledge base very straighforward.\n",
    "\n",
    "First, we need to create a collection, so Qdrant would know what vectors it will be dealing with, and then, we just pass our raw documents\n",
    "wrapped into `models.Document` to compute and upload the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43154775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.692231Z",
     "start_time": "2023-09-27T10:06:36.245915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = \"knowledge_base\"\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a71cb92-6033-4491-97c7-2649e056749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
    "    \"Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.\",\n",
    "    \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
    "    \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
    "    \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
    "    \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
    "    \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
    "    \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71e82dd-1077-4917-8811-3d819154b789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:56<00:00, 11.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=models.Document(text=document, model=model_name),\n",
    "            payload={\"document\": document},\n",
    "        )\n",
    "        for idx, document in enumerate(documents)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bddd6",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n",
    "RAG changes the way we interact with Large Language Models. We're converting a knowledge-oriented task, in which the model may create a counterfactual answer, into a language-oriented task. The latter expects the model to extract meaningful information and generate an answer. LLMs, when implemented correctly, are supposed to be carrying out language-oriented tasks.\n",
    "\n",
    "The task starts with the original prompt sent by the user. The same prompt is then vectorized and used as a search query for the most relevant facts. Those facts are combined with the original prompt to build a longer prompt containing more information.\n",
    "\n",
    "But let's start simply by asking our question directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed31ca63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.695165Z",
     "start_time": "2023-09-27T10:06:36.695150Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What tools should I need to use to build a web service using vector embeddings for search?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2d7dd",
   "metadata": {},
   "source": [
    "Using the Deepseek API requires providing the API key. You can obtain it from the [DeepSeek platform](https://platform.deepseek.com/api_keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c684",
   "metadata": {},
   "source": [
    "Now we can finally call the completion API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c12e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Fill the environmental variable with your own Deepseek API key\n",
    "# See: https://platform.deepseek.com/api_keys\n",
    "API_KEY = \"sk-9c78941ddcdb4f4ebca0ffea68214e2d\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def query_deepseek(prompt):\n",
    "    data = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.deepseek.com/chat/completions\", headers=HEADERS, data=json.dumps(data)\n",
    "    )\n",
    "\n",
    "    if response.ok:\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b08932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building a web service that leverages **vector embeddings** for search involves several components, from generating embeddings to storing and querying them efficiently. Here’s a breakdown of the essential tools and technologies you’ll need:\\n\\n---\\n\\n### **1. Embedding Generation**\\nTo convert text/data into vector embeddings, you\\'ll need:\\n- **Embedding Models**:\\n  - OpenAI\\'s `text-embedding-ada-002` (API-based, easy to use)\\n  - Open-source models like:\\n    - **Sentence Transformers** (e.g., `all-MiniLM-L6-v2`, `multi-qa-mpnet-base-dot-v1`)\\n    - **Hugging Face** models (e.g., `BAAI/bge-small-en`, `intfloat/e5-large`)\\n  - Cloud-based options:\\n    - Google\\'s Vertex AI Embeddings\\n    - Cohere Embed\\n    - AWS Bedrock (Titan Embeddings)\\n\\n- **Libraries**:\\n  - `sentence-transformers` (Python) for local models\\n  - `transformers` (Hugging Face) for custom models\\n  - OpenAI\\'s Python library for API-based embeddings\\n\\n---\\n\\n### **2. Vector Database (Storage & Search)**\\nTo store and query embeddings efficiently:\\n- **Dedicated Vector Databases**:\\n  - **Pinecone** (managed, easy to use)\\n  - **Weaviate** (open-source, supports hybrid search)\\n  - **Milvus** or **Zilliz** (scalable, open-source)\\n  - **Qdrant** (Rust-based, high performance)\\n  - **Chroma** (lightweight, good for prototyping)\\n  - **Redis with RedisSearch** (in-memory, supports vectors)\\n  - **PostgreSQL with pgvector** (if you prefer SQL)\\n\\n- **Cloud Options**:\\n  - AWS Aurora PostgreSQL (pgvector)\\n  - Google Vertex AI Matching Engine\\n  - Azure Cognitive Search (vector support)\\n\\n---\\n\\n### **3. Backend Framework**\\nTo build the web service:\\n- **Python** (popular for ML/data apps):\\n  - **FastAPI** or **Flask** (REST API)\\n  - **Django** (if you need a full-stack framework)\\n- **Node.js** (if you prefer JavaScript):\\n  - Express.js + vector DB client\\n- **Go/Rust** (for high-performance services)\\n\\n---\\n\\n### **4. Frontend (Optional)**\\nIf you need a UI for search:\\n- **React** / **Next.js** (modern frontend)\\n- **Vue.js** or **Svelte** (lighter alternatives)\\n- Libraries like `react-searchkit` or custom components.\\n\\n---\\n\\n### **5. Deployment & Scaling**\\n- **Containerization**: Docker + Kubernetes (for scaling)\\n- **Serverless**: AWS Lambda, Vercel, or Cloudflare Workers\\n- **Reverse Proxy**: Nginx or Caddy\\n- **Monitoring**: Prometheus + Grafana, or managed services (Datadog)\\n\\n---\\n\\n### **6. Additional Tools**\\n- **Caching**: Redis (for frequent queries)\\n- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana) or Loki\\n- **Auth**: JWT, OAuth (e.g., Auth0, Firebase Auth)\\n- **API Docs**: Swagger/OpenAPI (for FastAPI/Flask)\\n\\n---\\n\\n### **Example Workflow**\\n1. **Ingest Data**: Load text/data (e.g., CSV, JSON, or web scraping).\\n2. **Generate Embeddings**: Use Sentence Transformers or OpenAI.\\n3. **Store in Vector DB**: Index embeddings in Pinecone/Weaviate.\\n4. **Build API**: FastAPI endpoint to handle search queries.\\n5. **Query**: Convert user query → embedding → nearest-neighbor search.\\n6. **Deploy**: Dockerize and deploy on AWS/GCP.\\n\\n---\\n\\n### **Quick Start Example (Python)**\\n```python\\nfrom sentence_transformers import SentenceTransformer\\nimport pinecone\\n\\n# 1. Load embedding model\\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\\n\\n# 2. Connect to Pinecone\\npinecone.init(api_key=\"YOUR_KEY\")\\nindex = pinecone.Index(\"search-demo\")\\n\\n# 3. Store embeddings\\ntexts = [\"Hello world\", \"Machine learning is fun\"]\\nembeddings = model.encode(texts)\\nindex.upsert([(str(i), vec) for i, vec in enumerate(embeddings)])\\n\\n# 4. Query\\nquery_embedding = model.encode(\"What is fun?\")\\nresults = index.query(vector=query_embedding.tolist(), top_k=2)\\n```\\n\\n---\\n\\n### **Choosing Your Stack**\\n- **For simplicity**: OpenAI + Pinecone + FastAPI + Vercel.\\n- **For cost control**: Sentence Transformers + pgvector + Flask.\\n- **For scale**: Milvus/Zilliz + Kubernetes + Go.\\n\\nLet me know if you\\'d like a deeper dive into any of these areas!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_deepseek(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420d81d",
   "metadata": {},
   "source": [
    "### Extending the prompt\n",
    "\n",
    "Even though the original answer sounds credible, it didn't answer our question correctly. Instead, it gave us a generic description of an application stack. To improve the results, enriching the original prompt with the descriptions of the tools available seems like one of the possibilities. Let's use a semantic knowledge base to augment the prompt with the descriptions of different technologies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce791ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-27T10:06:36.702641Z",
     "start_time": "2023-09-27T10:06:36.702619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=0, version=0, score=0.67435956, payload={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=6, version=0, score=0.6314063, payload={'document': 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=5, version=0, score=0.6064709, payload={'document': 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=models.Document(text=prompt, model=model_name),\n",
    "    limit=3,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6640067",
   "metadata": {},
   "source": [
    "We used the original prompt to perform a semantic search over the set of tool descriptions. Now we can use these descriptions to augment the prompt and create more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16d8549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\\nSentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\\n\".join(r.payload['document'] for r in results.points)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c04a4e",
   "metadata": {},
   "source": [
    "Finally, let's build a metaprompt, the combination of the assumed role of the LLM, the original question, and the results from our semantic search that will force our LLM to use the provided context. \n",
    "\n",
    "By doing this, we effectively convert the knowledge-oriented task into a language task and hopefully reduce the chances of hallucinations. It also should make the response sound more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc9a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a software architect.\n",
      "Answer the following question using the provided context.\n",
      "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
      "\n",
      "Question: What tools should I need to use to build a web service using vector embeddings for search?\n",
      "\n",
      "Context:\n",
      "Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n",
      "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\n",
      "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaprompt = f\"\"\"\n",
    "You are a software architect.\n",
    "Answer the following question using the provided context.\n",
    "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "\n",
    "Question: {prompt.strip()}\n",
    "\n",
    "Context:\n",
    "{context.strip()}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Look at the full metaprompt\n",
    "print(metaprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a7678",
   "metadata": {},
   "source": [
    "Our current prompt is much longer, and we also used a couple of strategies to make the responses even better:\n",
    "\n",
    "1. The LLM has the role of software architect.\n",
    "2. We provide more context to answer the question.\n",
    "3. If the context contains no meaningful information, the model shouldn't make up an answer.\n",
    "\n",
    "Let's find out if that works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709b9f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To build a web service using vector embeddings for search, you can use the following tools based on the provided context:\\n\\n1. **SentenceTransformers**: For generating vector embeddings from text or sentences. This framework supports multiple languages and provides state-of-the-art embeddings for semantic search.\\n\\n2. **Qdrant**: As a vector database and similarity search engine. It allows you to store and efficiently search high-dimensional vectors (embeddings) generated by SentenceTransformers.\\n\\n3. **FastAPI**: As the web framework to build the API service. FastAPI is high-performance and easy to use, making it ideal for exposing search functionality to clients.\\n\\n### Workflow:\\n- Use **SentenceTransformers** to convert input text into embeddings.\\n- Store and index these embeddings in **Qdrant**.\\n- Build a **FastAPI** service to handle search requests, query Qdrant for similar vectors, and return results to the client.\\n\\nIf you need additional tools (e.g., for deployment or frontend), they are not mentioned in the context, so I don't know.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_deepseek(metaprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4120e1-9899-4caa-b974-51d9b3a485be",
   "metadata": {},
   "source": [
    "### Testing out the RAG pipeline\n",
    "\n",
    "By leveraging the semantic context we provided our model is doing a better job answering the question. Let's enclose the RAG as a function, so we can call it more easily for different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ed09d1-2c90-4ffc-9f1d-7beb87bab78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question: str, n_points: int = 3) -> str:\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(text=question, model=model_name),\n",
    "        limit=n_points,\n",
    "    )\n",
    "\n",
    "    context = \"\\n\".join(r.payload[\"document\"] for r in results.points)\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a software architect.\n",
    "    Answer the following question using the provided context.\n",
    "    If you can't find the answer, do not pretend you know it, but only answer \"I don't know\".\n",
    "\n",
    "    Question: {question.strip()}\n",
    "\n",
    "    Context:\n",
    "    {context.strip()}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    return query_deepseek(metaprompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fecd76-9a0b-4ad1-9097-b1d292a618ac",
   "metadata": {},
   "source": [
    "Now it's easier to ask a broad range of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa0fdead-a115-4fcd-88dc-5cc718dc0544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, a possible stack for a web API could include:\\n\\n1. **Web Framework**: FastAPI (for building the API with Python)\\n2. **Web Server/Reverse Proxy**: NGINX (to handle HTTP requests and serve as a reverse proxy)\\n3. **Database**: MySQL (as the relational database management system)\\n\\nThis stack combines FastAPI for high-performance API development, NGINX for efficient request handling and reverse proxying, and MySQL for structured data storage and retrieval. \\n\\nAdditional components (not mentioned in the context) might include Docker for containerization, Redis for caching, or other tools depending on the specific requirements. However, the context only provides information about FastAPI, NGINX, and MySQL.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"What can the stack for a web api look like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7324c127-c140-410a-ab19-87a5babce023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Where is the nearest grocery store?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56730-ed41-42c1-9c33-de3849c60b65",
   "metadata": {},
   "source": [
    "Our model can now:\n",
    "\n",
    "1. Take advantage of the knowledge in our vector datastore.\n",
    "2. Answer, based on the provided context, that it can not provide an answer.\n",
    "\n",
    "We have just shown a useful mechanism to mitigate the risks of hallucinations in Large Language Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
